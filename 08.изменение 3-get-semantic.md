Ага, теперь вижу весь файл — спасибо, что скинул. 

Твоя идея с `phoneme_vocab_size` **правильная, но она не чинит конкретную ошибку** на шаге 3. Ошибка сейчас из-за того, что мы грузим предобученный `s2G` с эмбеддингом на 732 токена в модель, у которой текстовый словарь уже 765. Нужно **выкинуть этот слой при загрузке**, потому что на шаге 3 он вообще не используется.

---

## Что нужно поменять в `3-get-semantic.py`

Сейчас в файле есть такой фрагмент:

```python
    print(
        vq_model.load_state_dict(
            torch.load(pretrained_s2G, map_location="cpu", weights_only=False)["weight"], strict=False
        )
    )
```

Его нужно **полностью заменить** на более аккуратную загрузку с фильтрацией слоя `enc_p.text_embedding.weight`.

Сделай так:

```python
    # Загружаем чекпоинт s2-модели
    state = torch.load(pretrained_s2G, map_location="cpu", weights_only=False)

    # В разных версиях checkpoint может быть либо сразу state_dict,
    # либо словарь с ключом "weight"
    weights = state.get("weight", state)

    # Если в чекпоинте есть старый эмбеддинг текста с другим размером —
    # выбрасываем его, он на этом шаге всё равно не используется.
    key = "enc_p.text_embedding.weight"
    if key in weights:
        w_ckpt = weights[key]
        w_model = vq_model.enc_p.text_embedding.weight
        if w_ckpt.shape != w_model.shape:
            print(
                "[3-get-semantic] Dropping",
                key,
                "due to size mismatch:",
                w_ckpt.shape,
                "vs",
                tuple(w_model.shape),
            )
            del weights[key]

    print(vq_model.load_state_dict(weights, strict=False))
```

То есть:

1. `torch.load` → `state`
2. `state.get("weight", state)` → `weights`
3. Если есть `enc_p.text_embedding.weight` и его размер не совпадает с текущей моделью → `del weights[key]`
4. `vq_model.load_state_dict(weights, strict=False)`

После замены **сохраняешь файл** и снова запускаешь:

```bat
"E:\Resound Project\GPT-SoVITS\venv_3.10\Scripts\python.exe" -s GPT_SoVITS/prepare_datasets/3-get-semantic.py
```

Если всё ок:

* не будет больше `size mismatch for enc_p.text_embedding.weight`,
* в `logs/ТВОЙ_ЭКСПЕРИМЕНТ/` появится `6-name2semantic-0.tsv` (и др., если частей несколько),
* WebUI перестанет ругаться на `FileNotFoundError: 'logs/.../6-name2semantic-0.tsv'`.

---

## Про `phoneme_vocab_size`

То, что ты написал:

> поменять `phoneme_vocab_size = 732` на `765`, так как добавили 33 буквы

Это **тоже нужно**, но для **другого места**:

* параметр `phoneme_vocab_size` в `configs/s1longer-v2.yaml` влияет на конфигурацию GPT-модели (s1),
  которая будет учиться на русском словаре;
* после наших правок `symbols2.symbols` действительно стал больше (старые символы + русские буквы),
  и его размер нужно подставить в `phoneme_vocab_size`.

Лучше всего:

1. В venv запустить:

   ```python
   from text import symbols2
   print(len(symbols2.symbols))
   ```

2. Взять полученное число и записать в `phoneme_vocab_size` в `s1longer-v2.yaml`.

Но **эта правка не заменяет патч в `3-get-semantic.py`** — там всё равно нужен трюк с выкидыванием старого слоя из checkpoint.

---

Если после правки 3-го шага будет новый лог или всё отработает, можешь кинуть финальный хвост вывода — глянем, всё ли дошло до конца и можно ли уже идти к обучению s1/s2.
