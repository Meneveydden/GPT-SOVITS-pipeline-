Смотри, по этим двум статьям картина такая:

* **`train-from-scratch`** — объясняет общую трёхстадийную схему обучения и чем оно отличается от веб-UI-файнтюна.([GitHub][1])
* **`how-to-train-the-models-with-other-languages`** — даёт конкретику, *какие именно файлы нужно править*, когда ты добавляешь новый язык (например, русский).([GitHub][2])

Ниже — сначала **короткий список файлов**, а потом **пошаговый план**, что с ними делать.

---

## 1. Какие файлы придётся менять

### 1. Текстовый фронтенд (фонемизация)

1. **`GPT_SoVITS/text/symbols.py`**

   * Добавить фонемы/символы нового языка (русского).([GitHub][2])

2. **`GPT_SoVITS/text/cleaner.py`**

   * Реализовать или подключить **g2p-функцию** для русского (text → phonemes).([GitHub][2])
   * Добавить язык в `language_module_map` (`"ru": "russian"` или как ты назовёшь модуль).([Hugging Face][3])
   * Убедиться, что `clean_text()` умеет принимать твой код языка и вызывать соответствующий модуль.([Hugging Face][3])
   * При необходимости создать файл `GPT_SoVITS/text/russian.py` (или аналогичный) с функциями `text_normalize()` и `g2p()`.

### 2. Конфиги модели

3. **`GPT_SoVITS/configs/s1longer-v2.yaml`**

   * Изменить `phoneme_vocab_size` под новое количество фонем (длина списка в `symbols.py`).([GitHub][2])

4. **`GPT_SoVITS/configs/s2.json`**

   * Для **обучения базовой модели (from scratch)**:

     * Разморозить VQ-квантизатор: `freeze_quantizer: false`.([GitHub][4])
     * Использовать нормальный LR для текстового энкодера (товарищи пишут, что веб-UI его специально занижает для fine-tune, а при base-тренинге этого не делают).([GitHub][1])

### 3. ASR для авто-разметки (если используешь встроенный ASR-tool)

5. **`tools/asr/config.py`**

   * В `asr_dict` добавить язык для Faster-Whisper, прописав нужный `lang` (например, `"ru"`).([GitHub][2])

### 4. Привязки к базовой модели и WebUI

6. **Все места, где жёстко прописан путь к базовой модели s1/s2**

   * Заменить путь на твой новый base/ru-base (в инференс-скриптах и/или webui-конфиге).([GitHub][2])

7. **WebUI (параллельный инференс нового языка)**

   * В интерфейсе и конфиге WebUI есть несколько мест, где перечислены доступные языки — их нужно дополнить твоим языком (на вики только скриншоты, но суть: добавить `"ru"` в выпадающие списки и внутренняя логика выбора языка).([GitHub][2])

---

## 2. План действий по шагам (под твою задачу «англ. → русский как родной»)

### Шаг 0. Решить стратегию

У тебя две опции:

1. **Расширить существующий многоязычный базовый модел (2k+ часов) под русский**
   — самый практичный путь: меньше данных, можно добиться «один и тот же голос говорит на EN и RU».

2. **Обучать всё «с нуля» с большим датасетом (100+ часов только русского + желательно мультиязычное окружение)**([GitHub][1])
   — дорого по данным и по GPU, но даёт «чистую» архитектуру.

Дальше план общий, отмечу где ветвления.

---

### Шаг 1. Подготовка данных

1. Собираешь **транскрибированные датасеты русского** (и при желании смешанные EN+RU для cross-lingual).

   * Вики рекомендует **≥100 часов**, если это *новый язык в базе*, а не просто мелкий финтюн.([GitHub][2])
2. Проверяешь качество:

   * чистый звук,
   * точные транскрипции,
   * одинаковый язык внутри семпла (без жёсткого code-switching),
   * нормализованный текст (без мусора) — см. требования к датасету в issue #176.([GitHub][4])

*(Это не правки файлов, но это критическое условие, иначе все последующие шаги смысла не имеют.)*

---

### Шаг 2. Добавить язык в текстовый фронтенд

**Файл 1: `GPT_SoVITS/text/symbols.py`**

1. Определяешь **набор фонем русского** (или букв + ударение, если делаешь упрощённо).
2. В `symbols.py` добавляешь:

   * новые отдельные списки фонем (если архитектура выделяет их по языкам),
   * или дописываешь общий список `symbols`/`phonemes` новыми элементами.([GitHub][2])
3. Считаешь итоговое количество символов -> запомни это число, оно пойдёт в `phoneme_vocab_size`.

**Файл 2: `GPT_SoVITS/text/cleaner.py`**

3. Создаёшь g2p-модуль для русского:

   * либо отдельный файл `GPT_SoVITS/text/russian.py` с функциями `text_normalize()` и `g2p()`,
   * либо используешь уже существующую либу (pysptk, phonemizer и т.п.) и оборачиваешь её там.
4. В `cleaner.py`:

   * в `language_module_map` добавляешь, например:

     ```python
     language_module_map = {
         "zh": "chinese2",
         "ja": "japanese",
         "en": "english",
         "ko": "korean",
         "yue": "cantonese",
         "ru": "russian",  # ← твой модуль
     }
     ```

     ([Hugging Face][3])
   * в логике `clean_text()` убеждаешься, что `'ru'` нормально прокидывается и модуль импортируется:

     ```python
     language_module = __import__("text." + language_module_map[language],
                                  fromlist=[language_module_map[language]])
     norm_text = language_module.text_normalize(text)
     phones = language_module.g2p(norm_text)
     ```
   * Проверяешь, что **каждая фонема** из `phones` есть в `symbols`, иначе будут assertion-ошибки.([Hugging Face][3])

---

### Шаг 3. Подогнать размеры словарей в конфиге

**Файл 3: `GPT_SoVITS/configs/s1longer-v2.yaml`**

1. Пересчитываешь `phoneme_vocab_size`:

   * это **кол-во фонем + спец-символы** (пауз, EOS и т.п.) — как у них сделано сейчас (см. пример конфига с `phoneme_vocab_size: 732`).([Hugging Face][5])
2. Заменяешь старое значение на новое.
3. Если ты **обучаешь базовую s1-модель с нуля**, этот конфиг будет использоваться и при тренировке, и при инференсе — обязательно держать его в синхроне с `symbols.py`.([GitHub][2])

---

### Шаг 4. Настройки s2 (SoVITS / vocoder)

**Файл 4: `GPT_SoVITS/configs/s2.json`**

1. Для **обучения «base model from scratch»**:

   * выставляешь `"freeze_quantizer": false`, чтобы VQ кодировщик обучался, а не был заморожен под старый языковой набор.([GitHub][1])
   * можешь поднять `text_low_lr_rate` до 1.0 (или близко к этому), чтобы текстовый энкодер не тормозился, как при fine-tune.([Hugging Face][6])
2. Если же ты **делаешь только финтюн от существующей base** (English+CN+JP+KR+YUE):

   * как раз наоборот: `"freeze_quantizer": true`,
   * `text_low_lr_rate` < 1, чтобы текстовый энкодер корректировался помягче.

*(В вики прямо написано: в WebUI-файнтюне VQ заморожен и LR понижен, а для base-тренинга — нет.)*([GitHub][1])

---

### Шаг 5. ASR-конфиг (если используешь автоматическую сегментацию/распознавание)

**Файл 5: `tools/asr/config.py`**

1. В `asr_dict` добавляешь конфиг для Faster-Whisper (или другого ASR), который умеет русский:

   ```python
   asr_dict = {
       "Faster Whisper (multi-lang)": {
           "lang": ["en", "zh", "ja", "ko", "ru"],  # добавляешь "ru"
           "model_path": "...",
           ...
       },
       ...
   }
   ```

   ([GitHub][2])
2. Если планируешь автоматически сегментировать английский и русский миксы, убедись, что выбранный ASR реально поддерживает обе.

---

### Шаг 6. Переопределить пути к базовым моделям

**Файл(ы): скрипты инференса / WebUI-конфиги**

1. Везде, где указаны оригинальные base-модели (`s1`/`s2`), меняешь пути на твои:

   * `s1_ckpt`: путь к твоей GPT-модели,
   * `s2_ckpt_dir`: каталог с твоими SoVITS-логами/чекпойнтами.([GitHub][2])
2. Это может быть:

   * `inference_webui.py` / `webui.py`,
   * `.env` или `configs/*.json` в папке проекта,
   * где-то в твоём собственном ланчере.

---

### Шаг 7. Поддержка языка в WebUI

1. В коде WebUI нужно:

   * добавить `"ru"` в список языков для инференса (там, где в интерфейсе ты выбираешь язык текста),
   * убедиться, что этот выбор прокидывается в `clean_text()` (то есть доходит до `language_module_map`).([GitHub][2])
2. На вики по этому поводу только скриншоты, поэтому конкретные файлы зависят от того, какую версию WebUI ты используешь (оригинал или чей-то форк), но паттерн один: ищешь массив вида `[("中文","zh"),("日本語","ja"),...]` и дописываешь туда русский.

---

### Шаг 8. Сам процесс обучения (3 стадии)

Это уже не про изменение файлов, но для полноты картины:

1. **S2 (token encoder/decoder)** — обучаешь токенизацию аудио (VQ), конфиг `configs/s2.json`.
2. **S1 (GPT)** — обучаешь модель, которая по фонемам/семантическим токенам генерирует соответствующую последовательность (конфиг `s1longer-v2.yaml`).([GitHub][1])
3. **(Опционально) S2+** — заморозив encoder s2, слегка дообучаешь decoder под свою задачу.([GitHub][1])

---

## 3. Как это привязать к твоей задаче (EN → RU «как родной»)

Мини-конкретика под твой use case:

* **Язык**:

  * добавляешь **русский** как новый язык (шаги 2–5),
  * при этом оставляешь английский как есть (мультиязычный фронтенд).

* **Данные**:

  * собираешь **русскую речь тех же или похожих голосов**, что и английская (идеально — один и тот же диктор на EN и RU),
  * используешь это для дообучения базовой модели (скорее вариант «файнтюн от 2k base», чем полностью с нуля).

* **Результат**:

  * модель знает фонетику английского и русского,
  * по английской аудиодорожке вытаскиваешь семантические токены/просодию,
  * подаёшь русский текст → g2p → GPT → SoVITS,
  * получаешь «тот же» голос, говорящий уже по-русски.

Если хочешь, следующим шагом можем прям разобрать:

1. какую архитектуру g2p для русского взять,
2. как минимально больно собрать 20–30 часов качественного русского датасета под конкретный голос (а не сразу 100+ часов).

[1]: https://github.com/RVC-Boss/GPT-SoVITS/wiki/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%28train-from-scratch%29 "从零训练(train from scratch) · RVC-Boss/GPT-SoVITS Wiki · GitHub"
[2]: https://github.com/RVC-Boss/GPT-SoVITS/wiki/%E8%AE%AD%E7%BB%83%E6%96%B0%E8%AF%AD%E8%A8%80%28how-to-train-the-models-with-other-languages%29 "训练新语言(how to train the models with other languages) · RVC-Boss/GPT-SoVITS Wiki · GitHub"
[3]: https://huggingface.co/spaces/lj1995/GPT-SoVITS-ProPlus/blob/main/text/cleaner.py?utm_source=chatgpt.com "text/cleaner.py · lj1995/GPT-SoVITS-ProPlus at main"
[4]: https://github.com/RVC-Boss/GPT-SoVITS/wiki/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%28train-from-scratch%29?utm_source=chatgpt.com "从零训练(train from scratch) · RVC-Boss/GPT-SoVITS Wiki"
[5]: https://huggingface.co/kevinwang676/GPT-SoVITS-v-3/blob/3a2b3624e90f380b0a875838523c3d6e4c19e8c7/GPT_SoVITS/configs/s1longer-v2.yaml?utm_source=chatgpt.com "GPT_SoVITS/configs/s1longer-v2.yaml · kevinwang676/ ..."
[6]: https://huggingface.co/spaces/lj1995/GPT-SoVITS-ProPlus/blob/main/configs/s2.json?utm_source=chatgpt.com "configs/s2.json · lj1995/GPT-SoVITS-ProPlus at main"
