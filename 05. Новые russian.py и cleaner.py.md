–°–¥–µ–ª–∞–ª–∞ —Ç–µ–±–µ —Ü–µ–ª—å–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç üòä
–ù–∏–∂–µ ‚Äî **–Ω–æ–≤—ã–π `russian.py`** –∏ **–Ω–æ–≤—ã–π `cleaner.py`**, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Å—Ç—ã–∫–æ–≤–∞—Ç—å—Å—è —Å —Ç–≤–æ–∏–º–∏ `symbols.py` / `symbols2.py` –∏ –Ω–µ –≤–∞–ª–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ.

---

## 1Ô∏è‚É£ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π `russian.py`

–≠—Ç–æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç:

* –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç **RUAccent + RUPhon** (–µ—Å–ª–∏ –æ–Ω–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã);
* –¥–∞—ë—Ç:

  * `text_normalize(text) -> str`
  * `g2p(text) -> List[str]` ‚Äî –∏–º–µ–Ω–Ω–æ —Ç–æ, —á—Ç–æ –∂–¥—ë—Ç `cleaner.py` –∏ `1-get-text.py`;
  * `g2p_with_word_info(...)` ‚Äî –Ω–∞ –±—É–¥—É—â–µ–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ-—Å–ª–æ–≤–Ω–∞—è –∏–Ω—Ñ–∞;
* —Å–æ–¥–µ—Ä–∂–∏—Ç **BERT-–±–ª–æ–∫**, –∫–æ—Ç–æ—Ä—ã–π:

  * **–ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏** `USE_RU_BERT=1` –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è,
  * –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –±–µ—Ä—ë—Ç—Å—è –∏–∑ `RU_BERT_DIR` (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é `GPT_SoVITS/pretrained_models/USER-bge-m3`),
  * –¥–∞—ë—Ç –∑–∞–≥–ª—É—à–∫—É `get_bert_feature(...)` —Å —Ä–∞–∑—É–º–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º.

–ü–æ–ª–æ–∂–∏ —ç—Ç–æ –≤ `GPT_SoVITS/text/russian.py`:

```python
# GPT_SoVITS/text/russian.py
"""
–†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –¥–ª—è GPT-SoVITS.

- text_normalize(text)  -> str
- g2p(text)             -> List[str]  (—Å–ø–∏—Å–æ–∫ —Ñ–æ–Ω–µ–º –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞ GPT-SoVITS)
- g2p_with_word_info    -> (phones, words, word2ph, string_mask)  (–¥–ª—è –±—É–¥—É—â–µ–≥–æ prosody/BERT)

BERT-–±–ª–æ–∫ (get_bert_feature) –≤–∫–ª—é—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏
–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è USE_RU_BERT == "1". –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ
–∑–∞–¥–∞—Ç—å —á–µ—Ä–µ–∑ RU_BERT_DIR. –≠—Ç–æ –Ω—É–∂–Ω–æ —É–∂–µ –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
–¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–∏, –Ω–æ –ù–ï —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TTS.
"""

import os
import re
from typing import List, Tuple, Optional

import torch

try:
    from ruphon import RUPhon
    from ruaccent import RUAccent
    from num2words import num2words
except Exception as e:  # –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
    print("[russian.py] Warning: failed to import RUPhon / RUAccent / num2words:", e)
    RUPhon = None
    RUAccent = None
    num2words = None

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –∏ –º–æ–¥–µ–ª–µ–π RUAccent / RUPhon ---

device = "cuda" if torch.cuda.is_available() else "cpu"

accentizer: Optional["RUAccent"] = None
phonemizer: Optional["RUPhon"] = None
models_loaded: bool = False

if RUAccent is not None and RUPhon is not None:
    try:
        accentizer = RUAccent()
        # turbo3 + —Å–ª–æ–≤–∞—Ä—å ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É/—Å–∫–æ—Ä–æ—Å—Ç–∏
        accentizer.load(omograph_model_size="turbo3", use_dictionary=True)

        phonemizer = RUPhon()
        models_loaded = True
        print("[russian.py] RUAccent + RUPhon initialized successfully.")
    except Exception as e:
        print("[russian.py] Warning: failed to init RUAccent/RUPhon:", e)
        accentizer = None
        phonemizer = None
        models_loaded = False
else:
    print("[russian.py] RUAccent/RUPhon not available, will fall back to simple char-level g2p.")

# --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ---

_digit_re = re.compile(r"\d+")


def _replace_number(match: re.Match) -> str:
    """–ó–∞–º–µ–Ω—è–µ–º —á–∏—Å–ª–∞ –Ω–∞ —Å–ª–æ–≤–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å num2words)."""
    if num2words is None:
        return match.group(0)
    try:
        value = int(match.group(0))
        return num2words(value, lang="ru")
    except Exception:
        return match.group(0)


def text_normalize(text: str) -> str:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:
    - –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    - –∑–∞–º–µ–Ω–∞ —ë -> –µ
    - —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª –≤ —Å–ª–æ–≤–∞
    - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è ¬´–ª–∏—à–Ω–∏—Ö¬ª —Å–∏–º–≤–æ–ª–æ–≤
    """
    if not isinstance(text, str):
        text = str(text)

    text = text.strip()
    if not text:
        return ""

    text = text.replace("–Å", "—ë")
    text = text.lower()
    # —á–∏—Å–ª–∞ -> —Å–ª–æ–≤–∞
    text = _digit_re.sub(_replace_number, text)

    # –æ—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –±–∞–∑–æ–≤—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"[^–∞-—è0-9 ,.!?\-]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# --- G2P ---

def _g2p_char_level(norm_text: str) -> List[str]:
    """
    –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: –ø–æ—Å–∏–º–≤–æ–ª—å–Ω—ã–π g2p, –µ—Å–ª–∏ RUAccent/RUPhon –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.
    –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —É–±–µ–¥–∏—Å—å, —á—Ç–æ –≤ symbols –¥–æ–±–∞–≤–ª–µ–Ω—ã –±—É–∫–≤—ã –∞-—è.
    """
    phones: List[str] = []
    for ch in norm_text:
        if ch == " ":
            continue
        phones.append(ch)
    return phones


def g2p_with_word_info(text: str) -> Tuple[List[str], List[str], Optional[List[int]], Optional[List[int]]]:
    """
    –ü–æ–ª–Ω–∞—è —Ñ–æ–Ω–µ–º–∏–∑–∞—Ü–∏—è: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–æ–Ω–µ–º—ã + –ø–æ-—Å–ª–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
    –ù—É–∂–Ω–∞ –≤ –±—É–¥—É—â–µ–º –¥–ª—è prosody/BERT. –í –ø–∞–π–ø–ª–∞–π–Ω–µ –æ–±—É—á–µ–Ω–∏—è TTS
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ g2p(), –∫–æ—Ç–æ—Ä–∞—è –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ —ç—Ç–∏–º –º–µ—Ç–æ–¥–æ–º.
    """
    norm_text = text_normalize(text)
    if not norm_text:
        return [], [], None, None

    if not models_loaded:
        phones = _g2p_char_level(norm_text)
        return phones, [], None, None

    try:
        # –°–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å—Ç–∞–≤–∏–º —É–¥–∞—Ä–µ–Ω–∏—è
        accented_text = accentizer.process_all(norm_text)
        # RUAccent —É–∂–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç '—ë', –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if not accented_text:
            accented_text = norm_text

        # RUPhon: –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–æ–Ω–µ–º –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å —Å–ª–æ–≤–∞–º–∏
        phonemes_list, words_list, word2ph, string_mask = phonemizer.phonemize_list(
            accented_text,
            put_stress=True,
            stress_symbol="'",
        )
        # phonemes_list ‚Äî –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —Ñ–æ–Ω–µ–º
        return phonemes_list, words_list, word2ph, string_mask
    except Exception as e:
        print(f"[russian.py] RUPhon error on text '{text}': {e}")
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –∫ –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–º—É —Ä–µ–∂–∏–º—É
        phones = _g2p_char_level(norm_text)
        return phones, [], None, None


def g2p(text: str) -> List[str]:
    """
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å english.g2p:
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ —Ñ–æ–Ω–µ–º –¥–ª—è cleaner.py.
    """
    phones, _words, _word2ph, _string_mask = g2p_with_word_info(text)
    return phones


# --- BERT (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Ç–æ–ª—å–∫–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞) ---

USE_RU_BERT = os.environ.get("USE_RU_BERT", "0") == "1"

bert_model = None
bert_tokenizer = None

if USE_RU_BERT:
    try:
        from transformers import AutoConfig, AutoModel, AutoTokenizer  # type: ignore

        # –ü—É—Ç—å –∫ RU-BERT / bge-m3-–º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è.
        bert_pretrained_dir = os.environ.get(
            "RU_BERT_DIR",
            "GPT_SoVITS/pretrained_models/USER-bge-m3",
        )

        print(f"[russian.py] Loading RU BERT model from: {bert_pretrained_dir}")
        bert_config = AutoConfig.from_pretrained(bert_pretrained_dir)
        bert_model = AutoModel.from_pretrained(
            bert_pretrained_dir,
            config=bert_config,
            trust_remote_code=True,
        )
        bert_tokenizer = AutoTokenizer.from_pretrained(bert_pretrained_dir)
        bert_model.to(device)
        bert_model.eval()
    except Exception as e:
        print("[russian.py] Warning: failed to load RU BERT model:", e)
        bert_model = None
        bert_tokenizer = None
        USE_RU_BERT = False


def get_bert_feature(
    norm_text: str,
    words: Optional[List[str]],
    word2ph: Optional[List[int]],
    string_mask: Optional[List[int]],
) -> Optional[torch.Tensor]:
    """
    –ß–µ—Ä–Ω–æ–≤–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ–ª—É—á–µ–Ω–∏—è BERT-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ.

    –û–∂–∏–¥–∞–µ–º, —á—Ç–æ:
      - norm_text ‚Äî —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç;
      - words / word2ph / string_mask ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã g2p_with_word_info
        (–ø–æ–∫–∞ –æ–Ω–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —É–ø–æ—Ä).

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä [hidden_size, N_phones] –∏–ª–∏ None,
    –µ—Å–ª–∏ BERT –æ—Ç–∫–ª—é—á—ë–Ω. –°–µ–π—á–∞—Å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≥—Ä—É–±–æ: –±–µ—Ä—ë–º
    —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤—Å–µ–π —Ñ—Ä–∞–∑—ã –∏ —Ç–∏—Ä–∞–∂–∏—Ä—É–µ–º –µ–≥–æ –¥–ª—è –≤—Å–µ—Ö —Ñ–æ–Ω–µ–º.
    –≠—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞–∫ –∑–∞–≥–ª—É—à–∫–∏; –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å
    –ø–æ–¥ —Ç–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ ¬´—Å–ª–æ–≤–æ ‚Üí —Ñ–æ–Ω–µ–º—ã¬ª.
    """
    if not USE_RU_BERT or bert_model is None or bert_tokenizer is None:
        return None

    if not norm_text:
        return None

    try:
        inputs = bert_tokenizer(
            norm_text,
            return_tensors="pt",
            truncation=True,
            max_length=256,
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = bert_model(**inputs)
        # [batch, seq_len, hidden]
        hidden_states = outputs.last_hidden_state  # type: ignore
        # —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º -> [batch, hidden]
        sent_emb = hidden_states.mean(dim=1)  # [1, hidden]
        sent_emb = sent_emb.squeeze(0)        # [hidden]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏–Ω—É –ø–æ —á–∏—Å–ª—É —Ñ–æ–Ω–µ–º: –µ—Å–ª–∏ word2ph –µ—Å—Ç—å, —Ç–æ –ø–æ —Å—É–º–º–µ;
        # –∏–Ω–∞—á–µ –ø—Ä–æ—Å—Ç–æ —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –¥–ª–∏–Ω–∞ = —á–∏—Å–ª—É —Å–∏–º–≤–æ–ª–æ–≤ norm_text.
        n_phones = 0
        if word2ph:
            n_phones = int(sum(word2ph))
        else:
            n_phones = max(1, len(norm_text.replace(" ", "")))

        # –¢–∏—Ä–∞–∂–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∞ –∫–∞–∂–¥—ã–π —Ñ–æ–Ω–µ–º
        phone_level_feature = sent_emb.unsqueeze(1).repeat(1, n_phones)
        # –í—ã—Ö–æ–¥: [hidden_size, N_phones]
        return phone_level_feature.cpu()
    except Exception as e:
        print("[russian.py] Warning: get_bert_feature failed:", e)
        return None


if __name__ == "__main__":  # –Ω–µ–±–æ–ª—å—à–∏–µ —Å–∞–º–æ—Ç–µ—Å—Ç—ã
    sample = "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! 123"
    print("Norm:", text_normalize(sample))
    print("Phones:", g2p(sample))
```

---

## 2Ô∏è‚É£ –ù–æ–≤—ã–π `cleaner.py`, –∑–∞—Ç–æ—á–µ–Ω–Ω—ã–π –ø–æ–¥ ru + —Ç–≤–æ–∏ symbols

–≠—Ç–æ—Ç `cleaner.py`:

* –Ω–µ —Ä–∞–∑–≤–∞–ª–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ª–∏—à–Ω–∏—Ö –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö;
* –¥–∞—ë—Ç —Ä–æ–≤–Ω–æ —Ç–æ, —á—Ç–æ –∂–¥—É—Ç:

  * `clean_text(text, language, version=None)` ‚Üí `(phones, word2ph, norm_text)`
  * `text_to_sequence(text, language, version=None)` ‚Üí `[int, int, ...]`
* –ø–æ–¥—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç `ru` —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å `text.russian`;
* –µ—Å–ª–∏ –∫–∞–∫–∞—è-—Ç–æ —Ñ–æ–Ω–µ–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ `symbols` / `symbols2`, –ø–∏—à–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å –∏ –∑–∞–º–µ–Ω—è–µ—Ç –Ω–∞ `UNK` (—á—Ç–æ–±—ã –ø–∞–π–ø–ª–∞–π–Ω —à—ë–ª –¥–∞–ª—å—à–µ, –∞ –Ω–µ –ø–∞–¥–∞–ª —Å Exception).

–ü–æ–ª–æ–∂–∏ —ç—Ç–æ –≤ `GPT_SoVITS/text/cleaner.py`:

```python
import os
from typing import List, Tuple, Optional

from text import cleaned_text_to_sequence
from text import symbols as symbols_v1
from text import symbols2 as symbols_v2

"""
–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –∏ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π cleaner.py,
–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.

–û—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã GPT-SoVITS:
- clean_text(text, language, version=None)
    -> (phones: List[str], word2ph: Optional[List[int]], norm_text: str)

- text_to_sequence(text, language, version=None)
    -> List[int] (id-—à–Ω–∏–∫–∏ —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ current symbols)

–î–ª—è zh / yue –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –±–∞–∑–æ–≤—ã–µ –≤–µ—Ç–∫–∏, –Ω–æ –ª–æ–≥–∏–∫–∞ SP2/SP3 –∏ –ø—Ä–æ—á–µ–≥–æ
—É–ø—Ä–æ—â–µ–Ω–∞, —Ç–∞–∫ –∫–∞–∫ –≤ —Ç–≤–æ—ë–º –∫–µ–π—Å–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ru + en.
"""


def _get_symbols_and_langmap(version: str):
    """
    –í—ã–±–æ—Ä —Å–ª–æ–≤–∞—Ä—è —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è language -> module name.
    """
    if version == "v1":
        symbols = symbols_v1.symbols
        language_module_map = {
            "zh": "chinese",
            "ja": "japanese",
            "en": "english",
        }
    else:
        symbols = symbols_v2.symbols
        language_module_map = {
            "zh": "chinese2",
            "ja": "japanese",
            "en": "english",
            "ko": "korean",
            "yue": "cantonese",
            "ru": "russian",  # <-- –Ω–æ–≤—ã–π —è–∑—ã–∫
        }
    return symbols, language_module_map


def clean_text(
    text: str,
    language: str,
    version: Optional[str] = None,
) -> Tuple[List[str], Optional[List[int]], str]:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç (text, language) –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–æ–Ω–µ–º.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - phones: —Å–ø–∏—Å–æ–∫ —Ñ–æ–Ω–µ–º (—Å—Ç—Ä–æ–∫),
      - word2ph: —Å–ø–∏—Å–æ–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤ —Ñ–æ–Ω–µ–º –Ω–∞ —Å–ª–æ–≤–æ (–∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è),
      - norm_text: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.

    –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç text/russian.py::g2p (—Å–ø–∏—Å–æ–∫ —Ñ–æ–Ω–µ–º).
    """
    if version is None:
        version = os.environ.get("version", "v2")

    symbols, language_module_map = _get_symbols_and_langmap(version)

    # –µ—Å–ª–∏ —è–∑—ã–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω ‚Äî —Å–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –≤ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
    if language not in language_module_map:
        print(f"[cleaner] Warning: unknown language '{language}', fallback to 'en'")
        language = "en"
        text = " "

    module_name = language_module_map[language]
    language_module = __import__(
        "text." + module_name,
        fromlist=[module_name],
    )

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏—è text_normalize)
    if hasattr(language_module, "text_normalize"):
        norm_text = language_module.text_normalize(text)
    else:
        norm_text = text

    # g2p: –¥–ª—è zh / yue –æ–∂–∏–¥–µ—Ç—Å—è (phones, word2ph),
    # –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö ‚Äî —Ç–æ–ª—å–∫–æ phones.
    if language in ("zh", "yue"):
        phones, word2ph = language_module.g2p(norm_text)
        try:
            assert len(phones) == sum(word2ph)
        except Exception:
            pass
    else:
        phones = language_module.g2p(norm_text)
        word2ph = None

    # —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å–ª–æ–≤–∞—Ä—é —Å–∏–º–≤–æ–ª–æ–≤
    phones_return: List[str] = []
    for ph in phones:
        if ph not in symbols:
            # –ï—Å–ª–∏ g2p –≤–µ—Ä–Ω—ë—Ç —Ñ–æ–Ω–µ–º—É, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ symbols2,
            # –æ–Ω–∞ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—Å—è –≤ UNK, –∞ –≤ –∫–æ–Ω—Å–æ–ª—å –≤—ã–≤–µ–¥–µ—Ç—Å—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ.
            print(f"[cleaner] ERROR: symbol '{ph}' not in symbols, replaced with 'UNK'")
            phones_return.append("UNK")
        else:
            phones_return.append(ph)

    return phones_return, word2ph, norm_text


def text_to_sequence(
    text: str,
    language: str,
    version: Optional[str] = None,
) -> List[int]:
    """
    –û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ clean_text, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
    """
    if version is None:
        version = os.environ.get("version", "v2")

    phones, _word2ph, _norm_text = clean_text(text, language, version)
    return cleaned_text_to_sequence(phones, version)


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    phones, w2p, norm = clean_text("–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! 123", "ru", "v2")
    print("Norm:", norm)
    print("Phones:", phones)
```

---

## 3Ô∏è‚É£ –ß—Ç–æ –¥–∞–ª—å—à–µ –¥–µ–ª–∞—Ç—å —Ä—É–∫–∞–º–∏

1. **–°–¥–µ–ª–∞—Ç—å –±—ç–∫–∞–ø** —Å—Ç–∞—Ä—ã—Ö `russian.py` –∏ `cleaner.py` (–ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∏–º–µ–Ω—É–π –∏—Ö –≤–æ —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ `*_old.py`).

2. –í—Å—Ç–∞–≤–∏—Ç—å —ç—Ç–∏ –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ –≤ `GPT_SoVITS/text/`.

3. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ `symbols2.py` **—Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —Ñ–æ–Ω–µ–º—ã**, –∫–æ—Ç–æ—Ä—ã–µ RUPhon –≤—ã–¥–∞—ë—Ç.
   –ï—Å–ª–∏ –ø—Ä–∏ –ø—Ä–æ–≥–æ–Ω–µ `1-get-text.py` —É–≤–∏–¥–∏—à—å:

   ```text
   [cleaner] ERROR: symbol '...' not in symbols, replaced with 'UNK'
   ```

   ‚Äî —ç—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ñ–æ–Ω–µ–º–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ `symbols2._russian_phonemes`. –¢–æ–≥–¥–∞:

   * –ª–∏–±–æ –¥–æ–±–∞–≤–ª—è–µ—à—å –µ—ë —Ç—É–¥–∞;
   * –ª–∏–±–æ (–µ—Å–ª–∏ –∏—Ö –º–∞–ª–æ) –º–æ–∂–µ—à—å –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–±–∏—Ç—å, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ –¢–¢–° –ø–æ —ç—Ç–∏–º —Ñ–æ–Ω–µ–º–∞–º –ø—Ä–æ—Å—è–¥–µ—Ç.

4. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–Ω–æ–≤–∞:

   ```bat
   "E:\Resound Project\GPT-SoVITS\venv_3.10\Scripts\python.exe" -s GPT_SoVITS/prepare_datasets/1-get-text.py
   ```

   –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, —á—Ç–æ —Ç–µ–ø–µ—Ä—å –Ω–µ—Ç `AssertionError` –∏–∑-–∑–∞ –ø—É—Å—Ç–æ–≥–æ `opt`.

---

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å, —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫–æ–π:
—Ç—ã –∑–∞–ø—É—Å–∫–∞–µ—à—å `1-get-text.py`, –∫–∏–¥–∞–µ—à—å –º–Ω–µ –ª–æ–≥ (–æ—Å–æ–±–µ–Ω–Ω–æ —Å—Ç—Ä–æ–∫–∏ `[cleaner] ERROR: ...`), –∞ —è –ø–æ–º–æ–≥—É –¥–æ–±–∏—Ç—å `symbols2.py` –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–æ–Ω–µ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª—å–Ω–æ –≤—ã–¥–∞—ë—Ç RUPhon.
